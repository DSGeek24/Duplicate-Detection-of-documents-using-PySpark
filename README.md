Given is a dataset which consists of large document collection. The problem is to find the most similar documents in this large dataset which can be achieved by a series of steps. Firstly, we find the most frequent words in the document collection followed by inverted index for each of them. Inverted index for each word is a list of tuples with filename with weight associated for each word. We then generate a similarity matrix for each pair of documents and fetch the most similar/identical documents based on the similarity value. This is useful in detecting duplicates in a document collection.  

For a large dataset, Apache Spark is a good choice as it is an open source distributed cluster-computing framework. It provides the advantages of data parallelism and fault tolerance where even if one worker crashes the tasks will be sent to different executors to be processed.  

The execution time to implement PySpark job to create an inverted index and calculate a similarity matrix using executors 5, 10 and 15 for different size datasets is found and compared. For each number of executors, 2 measurements were taken and the job which took least amount of time was considered since we canâ€™t conclude with one run as nodes on the cluster can be busy when many students are using it at the same time. 

The identical documents in the collection were identified as the ones with high similarity score. 
